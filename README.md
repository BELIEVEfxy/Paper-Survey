# Transformer and its Variants
## Structure
- ### *Attention is all you need*

- ### *Transformer-XL*

- ### *Star-Transformer*

- ### *Improving the Transformer Translation Model with Document-Level Context*

- ### *Area Attention*

- ### *Adaptive Attention Span in Transformers*

- ### *Input Combination Strategies for Multi-Source Transformer Decoder*

- ### *Scheduled Sampling for Transformers*

- ### *Character-Level Language Modeling with Deeper Self-Attention*

# Recommendation
## Traditional Sequential Recommendation
- ### *Session-Based Recommendations with Recurrent Neural Networks*

- ### *Recurrent neural networks with top-k gains for session-based recommendations*

## Transformer-based Recommendation

- ### *Self-Attentive Sequential Recommendation*

- ### *Behavior Sequence Transformer for E-commerce Recommendation in Alibaba*

- ### *BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer*

- ### *Deep Session Interest Network for Click-Through Rate Prediction*

## Repeat Consumption
- ### *Modeling Item-Specific Temporal Dynamics of Repeat Consumption for Recommender Systems*

- ### *RepeatNet: A Repeat Aware Neural Recommendation Machine for Session-based Recommendation*

- ### *The Dynamics of Repeat Consumption*

- ### *Incorporating Copying Mechanism in Sequence-to-Sequence Learning*

- ### *Pointer Networks*

## New-item Recommendation

- ### *New Item Consumption Prediction Using Deep Learning*

- ### *Fashion Retail: Forecasting Demand for New Items*
